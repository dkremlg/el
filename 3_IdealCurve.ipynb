{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1234,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pandas import Series\n",
    "from pandas import DataFrame\n",
    "\n",
    "from scipy.stats import poisson\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1235,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='C:/Users/35266/Documents/Python Scripts/el/'\n",
    "route='LUX-TXL'\n",
    "risk_spill=0.8\n",
    "risk_spoil=risk_spill\n",
    "unique_identifier=['DepDate','FltNum','dtime']\n",
    "cluster_variables=['dday','dtime','Direction','month']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine downweight factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1236,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_datadriven=pd.read_csv(path+'R_Training_Pax.csv')\n",
    "TotalCap=pd.read_csv(path+'Capacity_forPlots_'+route+'.csv')\n",
    "Prediction=pd.read_csv(path+'R_Output_Test_Pax.csv',sep=',')\n",
    "GroupPax=pd.read_csv(path+'FrontEnd_Input/GroupPax_'+route+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1237,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_datadriven=C_datadriven.groupby(list(set(unique_identifier+cluster_variables)))['NumPax'].sum().reset_index()\n",
    "\n",
    "deptime=[str(x/60).split('.')[0]+':'+str(round(float('0.'+str(x/60).split('.')[1])*60)) for x in C_datadriven['dtime'].unique()]\n",
    "deptime=[x+'0' if len(x.split(':')[1])==1 else x for x in deptime]\n",
    "deptime=['0'+x if len(x.split(':')[0])==1 else x for x in deptime]\n",
    "\n",
    "Map_DepTime=DataFrame([C_datadriven['dtime'].unique(),[x+'0' if len(x.split(':')[1])==1 else x for x in deptime]]).transpose()\n",
    "Map_DepTime.columns=['dtime','deptime']\n",
    "Map_DepTime['dtime']=Map_DepTime['dtime'].astype('int')\n",
    "\n",
    "C_datadriven=C_datadriven.merge(Map_DepTime,on='dtime')\n",
    "C_datadriven['dtime']=C_datadriven['deptime']\n",
    "C_datadriven=C_datadriven[[x for x in C_datadriven.columns if x!='deptime']]\n",
    "\n",
    "C_datadriven=C_datadriven.merge(TotalCap.loc[TotalCap['Dprio']==1,:],on=unique_identifier)\n",
    "\n",
    "C_datadriven['downweight']=C_datadriven['NumPax']/C_datadriven['Cabin Capacity']\n",
    "C_datadriven=C_datadriven.groupby(cluster_variables)['downweight'].apply(lambda x: np.quantile(x,0.5)).reset_index()\n",
    "C_datadriven['downweight']=C_datadriven['downweight'].apply(lambda x: 0.8 if x>0.8 else x)\n",
    "C_datadriven['downweight']=C_datadriven['downweight'].apply(lambda x: 0.2 if x<0.2 else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import to total capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Associate downweight factor and cabin capacity to prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1238,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'dday'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1238-a35d1e5f1942>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m on=unique_identifier+['Dprio'])\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mPrediction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPrediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC_datadriven\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcluster_variables\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mPrediction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'downweight'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPrediction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'downweight'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.8\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m   6387\u001b[0m                      \u001b[0mright_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mright_on\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mleft_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6388\u001b[0m                      \u001b[0mright_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mright_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6389\u001b[1;33m                      copy=copy, indicator=indicator, validate=validate)\n\u001b[0m\u001b[0;32m   6390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6391\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecimals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m     59\u001b[0m                          \u001b[0mright_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mright_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m                          \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindicator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                          validate=validate)\n\u001b[0m\u001b[0;32m     62\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    549\u001b[0m         (self.left_join_keys,\n\u001b[0;32m    550\u001b[0m          \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m          self.join_names) = self._get_merge_keys()\n\u001b[0m\u001b[0;32m    552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m         \u001b[1;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_get_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    855\u001b[0m                             right_keys.append(\n\u001b[0;32m    856\u001b[0m                                 right._get_label_or_level_values(\n\u001b[1;32m--> 857\u001b[1;33m                                     rk, stacklevel=stacklevel))\n\u001b[0m\u001b[0;32m    858\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m                             \u001b[1;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[1;34m(self, key, axis, stacklevel)\u001b[0m\n\u001b[0;32m   1380\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1381\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1382\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1384\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'dday'"
     ]
    }
   ],
   "source": [
    "deptime=[str(x/60).split('.')[0]+':'+str(round(float('0.'+str(x/60).split('.')[1])*60)) for x in Prediction['dtime'].unique()]\n",
    "deptime=[x+'0' if len(x.split(':')[1])==1 else x for x in deptime]\n",
    "deptime=['0'+x if len(x.split(':')[0])==1 else x for x in deptime]\n",
    "\n",
    "Map_DepTime=DataFrame([Prediction['dtime'].unique(),[x+'0' if len(x.split(':')[1])==1 else x for x in deptime]]).transpose()\n",
    "Map_DepTime.columns=['dtime','deptime']\n",
    "Map_DepTime['dtime']=Map_DepTime['dtime'].astype('int')\n",
    "\n",
    "Prediction=Prediction.merge(Map_DepTime,on='dtime')\n",
    "Prediction['dtime']=Prediction['deptime']\n",
    "Prediction=Prediction[[x for x in Prediction.columns if x!='deptime']]\n",
    "\n",
    "Prediction=Prediction.merge(TotalCap[unique_identifier+['Dprio','Cabin Capacity']],\n",
    "on=unique_identifier+['Dprio'])\n",
    "\n",
    "Prediction=Prediction.merge(C_datadriven,on=cluster_variables,how='left')\n",
    "Prediction['downweight']=Prediction['downweight'].apply(lambda x: 0.8 if pd.isnull(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IdealCurve=Prediction[unique_identifier+['Dprio','forecast_bookings','Cabin Capacity','downweight']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IdealCurve=IdealCurve.merge(IdealCurve.groupby(unique_identifier)['forecast_bookings'].sum().reset_index()\\\n",
    ".rename(columns={'forecast_bookings': 'forecast_bookings_sum'}),on=['DepDate','FltNum','dtime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IdealCurve['forecast_bookings']=IdealCurve['forecast_bookings']/IdealCurve['forecast_bookings_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IdealCurve=IdealCurve[[x for x in IdealCurve.columns if x!='forecast_bookings_sum']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IdealCurve['lambda_100']=IdealCurve['Cabin Capacity']*IdealCurve['forecast_bookings']\n",
    "IdealCurve['lambda_80']=IdealCurve['Cabin Capacity']*IdealCurve['downweight']*IdealCurve['forecast_bookings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IdealCurve=IdealCurve[unique_identifier+['Dprio']+[x for x in IdealCurve.columns if 'lambda' in x]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IdealCurve['Dprio']=-IdealCurve['Dprio']\n",
    "IdealCurve=IdealCurve.set_index(['Dprio']+unique_identifier)\n",
    "\n",
    "IdealCurve_cumul_100=IdealCurve.groupby(level=[x for x in range(len(unique_identifier)+1)]).sum()\\\n",
    ".groupby(level=[x for x in range(1,len(unique_identifier)+1)])['lambda_100'].cumsum().reset_index().rename(columns={'lambda_100': 'lambda_100_cumsum'})\n",
    "IdealCurve_cumul_100['Dprio']=-IdealCurve_cumul_100['Dprio']\n",
    "\n",
    "IdealCurve_cumul_80=IdealCurve.groupby(level=[x for x in range(len(unique_identifier)+1)]).sum()\\\n",
    ".groupby(level=[x for x in range(1,len(unique_identifier)+1)])['lambda_80'].cumsum().reset_index().rename(columns={'lambda_80': 'lambda_80_cumsum'})\n",
    "IdealCurve_cumul_80['Dprio']=-IdealCurve_cumul_80['Dprio']\n",
    "\n",
    "IdealCurve=IdealCurve.reset_index()\n",
    "IdealCurve['Dprio']=-IdealCurve['Dprio']\n",
    "\n",
    "IdealCurve=IdealCurve.set_index(['Dprio']+unique_identifier)\n",
    "\n",
    "IdealCurve_cond_100=IdealCurve.groupby(level=[x for x in range(len(unique_identifier)+1)]).sum()\\\n",
    ".groupby(level=[x for x in range(1,len(unique_identifier)+1)])['lambda_100'].cumsum().reset_index().rename(columns={'lambda_100': 'lambda_100_cond'})\n",
    "\n",
    "IdealCurve_cond_80=IdealCurve.groupby(level=[x for x in range(len(unique_identifier)+1)]).sum()\\\n",
    ".groupby(level=[x for x in range(1,len(unique_identifier)+1)])['lambda_80'].cumsum().reset_index().rename(columns={'lambda_80': 'lambda_80_cond'})\n",
    "\n",
    "IdealCurve=IdealCurve.reset_index()\n",
    "\n",
    "IdealCurve_cumul_100=IdealCurve_cumul_100.sort_values(by=['Dprio']+unique_identifier)\n",
    "IdealCurve_cumul_80=IdealCurve_cumul_80.sort_values(by=['Dprio']+unique_identifier)\n",
    "IdealCurve_cond_100=IdealCurve_cond_100.sort_values(by=['Dprio']+unique_identifier)\n",
    "IdealCurve_cond_100['thres_100']=poisson.ppf(1-risk_spoil,IdealCurve_cond_100['lambda_100_cond'])\n",
    "IdealCurve_cond_80=IdealCurve_cond_80.sort_values(by=['Dprio']+unique_identifier)\n",
    "IdealCurve_cond_80['thres_80']=poisson.ppf(risk_spoil,IdealCurve_cond_80['lambda_80_cond'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IdealCurve_cond_100=IdealCurve_cond_100\\\n",
    ".merge(Prediction[['Dprio']+unique_identifier+['Cabin Capacity']],on=['Dprio']+unique_identifier)\n",
    "\n",
    "IdealCurve_cond_80=IdealCurve_cond_80\\\n",
    ".merge(Prediction[['Dprio']+unique_identifier+['Cabin Capacity','downweight']],on=['Dprio']+unique_identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IdealCurve_cond_100['Ideal_upper']=IdealCurve_cond_100['Cabin Capacity']-IdealCurve_cond_100['thres_100']\n",
    "IdealCurve_cond_100['Ideal_upper']=IdealCurve_cond_100[['Ideal_upper','Cabin Capacity']].apply(lambda x: x[1] if x[0]>x[1] else x[0],axis=1)\n",
    "IdealCurve_cond_80['Ideal_lower']=IdealCurve_cond_80['Cabin Capacity']*IdealCurve_cond_80['downweight']-IdealCurve_cond_80['thres_80']\n",
    "IdealCurve_cond_80['Ideal_lower']=IdealCurve_cond_80['Ideal_lower'].apply(lambda x: 0 if x<0 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IdealCurve=IdealCurve_cumul_100.merge(IdealCurve_cumul_80,on=['Dprio']+unique_identifier)\\\n",
    ".merge(IdealCurve_cond_80[['Dprio']+unique_identifier+['Ideal_lower']],on=['Dprio']+unique_identifier)\\\n",
    ".merge(IdealCurve_cond_100[['Dprio']+unique_identifier+['Ideal_upper']],on=['Dprio']+unique_identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IdealCurve=IdealCurve[['Dprio']+unique_identifier+['Ideal_lower','lambda_80_cumsum','lambda_100_cumsum','Ideal_upper']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IdealCurve.columns=['Dprio']+unique_identifier+['Ramp-up frontier','Ideal curve (80% LF)','Ideal curve (100% LF)','Phase-down frontier']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove impact of group pax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IdealCurve=IdealCurve.merge(GroupPax[['Dprio']+unique_identifier+['Group_pax_cumul']],on=['Dprio']+unique_identifier)\n",
    "\n",
    "IdealCurve=IdealCurve.merge(TotalCap[['Dprio']+unique_identifier+['Cabin Capacity']],on=['Dprio']+unique_identifier)\n",
    "\n",
    "IdealCurve=IdealCurve.merge(IdealCurve.loc[IdealCurve['Dprio']==1,unique_identifier+['Ideal curve (80% LF)','Ideal curve (100% LF)']]\\\n",
    ".rename(columns={'Ideal curve (100% LF)': 'C_100', 'Ideal curve (80% LF)': 'C_80'}),on=unique_identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IdealCurve['Ideal curve (100% LF)']=IdealCurve['Ideal curve (100% LF)']\\\n",
    "*((IdealCurve['Cabin Capacity']-IdealCurve['Group_pax_cumul'])/IdealCurve['Cabin Capacity'])\n",
    "IdealCurve['Phase-down frontier']=IdealCurve['Phase-down frontier']\\\n",
    "*((IdealCurve['Cabin Capacity']-IdealCurve['Group_pax_cumul'])/IdealCurve['Cabin Capacity'])\n",
    "# IdealCurve['Ideal curve (80% LF)']=IdealCurve[['Ideal curve (80% LF)','Group_pax_cumul','C_100','C_80']]\\\n",
    "# .apply(lambda x: x[0] if x[2]-x[1]>x[3] else x[0]-(x[3]-(x[2]-x[1])),axis=1)\n",
    "# IdealCurve['Ramp-up frontier']=IdealCurve[['Ramp-up frontier','Group_pax_cumul','C_100','C_80']]\\\n",
    "# .apply(lambda x: x[0] if x[2]-x[1]>x[3] else x[0]-(x[3]-(x[2]-x[1])),axis=1)\n",
    "IdealCurve['Ideal curve (80% LF)']=IdealCurve['Ideal curve (80% LF)']\\\n",
    "*((IdealCurve['Cabin Capacity']-IdealCurve['Group_pax_cumul'])/IdealCurve['Cabin Capacity'])\n",
    "IdealCurve['Ramp-up frontier']=IdealCurve['Ramp-up frontier']\\\n",
    "*((IdealCurve['Cabin Capacity']-IdealCurve['Group_pax_cumul'])/IdealCurve['Cabin Capacity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IdealCurve['Ideal curve (100% LF)']=IdealCurve['Ideal curve (100% LF)'].apply(lambda x: 0 if x<0 else x)\n",
    "IdealCurve['Phase-down frontier']=IdealCurve['Phase-down frontier'].apply(lambda x: 0 if x<0 else x)\n",
    "IdealCurve['Ideal curve (80% LF)']=IdealCurve['Ideal curve (80% LF)'].apply(lambda x: 0 if x<0 else x)\n",
    "IdealCurve['Ramp-up frontier']=IdealCurve['Ramp-up frontier'].apply(lambda x: 0 if x<0 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IdealCurve=IdealCurve[[x for x in IdealCurve.columns if 'cumul' not in x and 'C_' not in x and 'Capacity' not in x]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Risk and Actual Bookings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Risk=Prediction[['Dprio']+unique_identifier+['NumPax']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mapping_StrToDt=pd.concat([Series(Risk['DepDate'].unique()),\n",
    "Series([pd.to_datetime(x) for x in Risk['DepDate'].unique()])],axis=1)\n",
    "Mapping_StrToDt.columns=['DepDate','DepDate_new']\n",
    "Risk=Risk.merge(Mapping_StrToDt,on='DepDate')\n",
    "Risk['DepDate']=Risk['DepDate_new']\n",
    "Risk=Risk[[x for x in Risk.columns if x!='DepDate_new']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Risk['Dprio']=-Risk['Dprio']\n",
    "Risk=Risk.set_index(['Dprio']+unique_identifier)\n",
    "\n",
    "Risk=Risk.groupby(level=[x for x in range(len(unique_identifier)+1)]).sum()\\\n",
    ".groupby(level=[x for x in range(1,len(unique_identifier)+1)])['NumPax'].cumsum().reset_index()\\\n",
    ".rename(columns={'NumPax': 'NumPax_cumsum'})\n",
    "\n",
    "Risk['Dprio']=-Risk['Dprio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Risk['IssueDate']=Risk['DepDate']-Risk['Dprio'].apply(lambda x: datetime.timedelta(x-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermezzo for creating Actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Actuals=Risk.loc[Risk['IssueDate']<=pd.to_datetime(datetime.datetime.today().strftime('%Y-%m-%d')),:]\n",
    "Actuals=Actuals[[x for x in Actuals.columns if x!='IssueDate']]\n",
    "Actuals=Actuals.rename(columns={'NumPax_cumsum': 'Actual Bookings'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue with Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Risk=Risk.loc[Risk['IssueDate']==pd.to_datetime(datetime.datetime.today().strftime('%Y-%m-%d')),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Risk=Risk[[x for x in Risk.columns if x!='IssueDate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Risk['DepDate']=Risk['DepDate'].apply(lambda x: str(x).split(' ')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Risk=Risk.merge(IdealCurve.loc[IdealCurve['Dprio']==1,unique_identifier+['Ideal curve (80% LF)','Ideal curve (100% LF)']]\\\n",
    ".rename(columns={'Ideal curve (100% LF)': 'C_100', 'Ideal curve (80% LF)': 'C_80'}),on=unique_identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Risk=Risk.merge(IdealCurve_cond_100[['Dprio']+unique_identifier+['lambda_100_cond']],on=['Dprio']+unique_identifier)\\\n",
    ".merge(IdealCurve_cond_80[['Dprio']+unique_identifier+['lambda_80_cond']],on=['Dprio']+unique_identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Risk['SpillageRisk']=1-poisson.cdf(Risk['C_100']-Risk['NumPax_cumsum'],Risk['lambda_100_cond'])\n",
    "Risk['SpoilageRisk']=poisson.cdf(Risk['C_80']-Risk['NumPax_cumsum'],Risk['lambda_80_cond'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Risk=Risk.merge(IdealCurve[['Dprio']+unique_identifier+['Ramp-up frontier','Phase-down frontier']],\n",
    "          on=['Dprio']+unique_identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Risk['Intensity_downweighted']=Risk['NumPax_cumsum']-Risk['Ramp-up frontier']\n",
    "Risk['Intensity_full']=Risk['NumPax_cumsum']-Risk['Phase-down frontier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Risk=Risk[['DepDate','FltNum','dtime','SpoilageRisk','Intensity_downweighted','Intensity_full','SpillageRisk']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TotalCap=TotalCap.merge(GroupPax[['Dprio']+unique_identifier+['Group_pax_cumul']],on=['Dprio']+unique_identifier)\n",
    "\n",
    "TotalCap['Real Cabin Capacity']=TotalCap['Cabin Capacity']-TotalCap['Group_pax_cumul']\n",
    "TotalCap=TotalCap.rename(columns={'Cabin Capacity': 'Initial Cabin Capacity'})\n",
    "\n",
    "TotalCap.to_csv(path+'FrontEnd_Input/Capacity_'+route+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Risk=Risk.merge(Prediction[unique_identifier+['Direction','month','dday']].drop_duplicates(subset=unique_identifier),on=unique_identifier)\n",
    "Risk['Route']=route\n",
    "\n",
    "MonthMapping=DataFrame([[x for x in range(1,13)],['January','February','March','April','May','June','July','August',\n",
    "                'September','October','November','December']]).transpose()\n",
    "MonthMapping.columns=['month','Month']\n",
    "MonthMapping['month']=MonthMapping['month'].astype('int') \n",
    "Risk=Risk.merge(MonthMapping,on='month')\n",
    "Risk['month']=Risk['Month']\n",
    "Risk=Risk[[x for x in Risk.columns if x!='Month']]\n",
    "\n",
    "Risk=Risk[['Route','DepDate', 'FltNum', 'dtime', 'Direction', 'month', 'dday',\n",
    "       'SpoilageRisk', 'Intensity_downweighted', 'Intensity_full',\n",
    "       'SpillageRisk']]\n",
    "\n",
    "Risk=Risk.sort_values(by=['DepDate', 'FltNum', 'dtime', 'Direction'])\n",
    "\n",
    "Risk.to_csv(path+'FrontEnd_Input/Risk_'+route+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Actuals['DepDate']=Actuals['DepDate'].apply(lambda x: str(x).split(' ')[0])\n",
    "\n",
    "Actuals=Actuals.merge(Prediction[unique_identifier+['Direction','month','dday']].drop_duplicates(subset=unique_identifier),\n",
    "on=unique_identifier)\n",
    "Actuals['Route']=route\n",
    "\n",
    "Actuals=Actuals[['Route','DepDate', 'FltNum', 'dtime', 'Direction', 'month', 'dday', 'Dprio','Actual Bookings']]\n",
    "\n",
    "Actuals['Actual Bookings']=Actuals['Actual Bookings'].apply(lambda x: x if x>=0 else 0)\n",
    "\n",
    "Actuals.to_csv(path+'FrontEnd_Input/Actuals_'+route+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IdealCurve=IdealCurve.merge(Prediction[unique_identifier+['Direction','month','dday']].drop_duplicates(subset=unique_identifier),\n",
    "on=unique_identifier)\n",
    "\n",
    "IdealCurve['Route']=route\n",
    "\n",
    "IdealCurve=IdealCurve[['Route','DepDate', 'FltNum', 'dtime', 'Direction', 'month', 'dday', 'Dprio',\n",
    "'Ramp-up frontier','Ideal curve (80% LF)','Ideal curve (100% LF)','Phase-down frontier']]\n",
    "\n",
    "IdealCurve.to_csv(path+'FrontEnd_Input/IdealCurve_'+route+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
