{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pandas import Series\n",
    "from pandas import DataFrame\n",
    "\n",
    "from scipy.stats import poisson\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='C:/Users/35266/Documents/Python Scripts/el/'\n",
    "route='SCN-TXL'\n",
    "risk_spill=0.8\n",
    "risk_spoil=risk_spill\n",
    "unique_identifier=['DepDate','FltNum','dtime']\n",
    "cluster_variables=['dday','dtime','Direction','month']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine downweight factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_datadriven=pd.read_csv(path+'Intermediate_Output/R_Training_Pax.csv')\n",
    "TotalCap=pd.read_csv(path+'Intermediate_Output/Capacity_forPlots_'+route+'.csv')\n",
    "Prediction=pd.read_csv(path+'Intermediate_Output/R_Output_Test_Pax.csv',sep=',')\n",
    "GroupPax=pd.read_csv(path+'FrontEnd_Input/GroupPax_'+route+'.csv')\n",
    "\n",
    "C_datadriven=C_datadriven.groupby(list(set(unique_identifier+cluster_variables)))['NumPax'].sum().reset_index()\n",
    "\n",
    "deptime=[str(x/60).split('.')[0]+':'+str(round(float('0.'+str(x/60).split('.')[1])*60)) for x in C_datadriven['dtime'].unique()]\n",
    "deptime=[x+'0' if len(x.split(':')[1])==1 else x for x in deptime]\n",
    "deptime=['0'+x if len(x.split(':')[0])==1 else x for x in deptime]\n",
    "\n",
    "Map_DepTime=DataFrame([C_datadriven['dtime'].unique(),[x+'0' if len(x.split(':')[1])==1 else x for x in deptime]]).transpose()\n",
    "Map_DepTime.columns=['dtime','deptime']\n",
    "Map_DepTime['dtime']=Map_DepTime['dtime'].astype('int')\n",
    "\n",
    "C_datadriven=C_datadriven.merge(Map_DepTime,on='dtime')\n",
    "C_datadriven['dtime']=C_datadriven['deptime']\n",
    "C_datadriven=C_datadriven[[x for x in C_datadriven.columns if x!='deptime']]\n",
    "\n",
    "C_datadriven=C_datadriven.merge(TotalCap.loc[TotalCap['Dprio']==1,:],on=unique_identifier)\n",
    "\n",
    "C_datadriven['downweight']=C_datadriven['NumPax']/C_datadriven['Cabin Capacity']\n",
    "C_datadriven=C_datadriven.groupby(cluster_variables)['downweight'].apply(lambda x: np.quantile(x,0.5)).reset_index()\n",
    "C_datadriven['downweight']=C_datadriven['downweight'].apply(lambda x: 0.8 if x>0.8 else x)\n",
    "C_datadriven['downweight']=C_datadriven['downweight'].apply(lambda x: 0.2 if x<0.2 else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Associate downweight factor and cabin capacity to prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "deptime=[str(x/60).split('.')[0]+':'+str(round(float('0.'+str(x/60).split('.')[1])*60)) for x in Prediction['dtime'].unique()]\n",
    "deptime=[x+'0' if len(x.split(':')[1])==1 else x for x in deptime]\n",
    "deptime=['0'+x if len(x.split(':')[0])==1 else x for x in deptime]\n",
    "\n",
    "Map_DepTime=DataFrame([Prediction['dtime'].unique(),[x+'0' if len(x.split(':')[1])==1 else x for x in deptime]]).transpose()\n",
    "Map_DepTime.columns=['dtime','deptime']\n",
    "Map_DepTime['dtime']=Map_DepTime['dtime'].astype('int')\n",
    "\n",
    "Prediction=Prediction.merge(Map_DepTime,on='dtime')\n",
    "Prediction['dtime']=Prediction['deptime']\n",
    "Prediction=Prediction[[x for x in Prediction.columns if x!='deptime']]\n",
    "\n",
    "Prediction=Prediction.merge(TotalCap[unique_identifier+['Dprio','Cabin Capacity']],\n",
    "on=unique_identifier+['Dprio'])\n",
    "\n",
    "Prediction=Prediction.merge(C_datadriven,on=cluster_variables,how='left')\n",
    "Prediction['downweight']=Prediction['downweight'].apply(lambda x: 0.8 if pd.isnull(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "IdealCurve=Prediction[unique_identifier+['Dprio','forecast_bookings','Cabin Capacity','downweight']].copy()\n",
    "\n",
    "IdealCurve=IdealCurve.merge(IdealCurve.groupby(unique_identifier)['forecast_bookings'].sum().reset_index()\\\n",
    ".rename(columns={'forecast_bookings': 'forecast_bookings_sum'}),on=['DepDate','FltNum','dtime'])\n",
    "\n",
    "IdealCurve['forecast_bookings']=IdealCurve['forecast_bookings']/IdealCurve['forecast_bookings_sum']\n",
    "\n",
    "IdealCurve=IdealCurve[[x for x in IdealCurve.columns if x!='forecast_bookings_sum']]\n",
    "\n",
    "IdealCurve['lambda_100']=IdealCurve['Cabin Capacity']*IdealCurve['forecast_bookings']\n",
    "IdealCurve['lambda_80']=IdealCurve['Cabin Capacity']*IdealCurve['downweight']*IdealCurve['forecast_bookings']\n",
    "\n",
    "IdealCurve=IdealCurve[unique_identifier+['Dprio']+[x for x in IdealCurve.columns if 'lambda' in x]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "IdealCurve['Dprio']=-IdealCurve['Dprio']\n",
    "IdealCurve=IdealCurve.set_index(['Dprio']+unique_identifier)\n",
    "\n",
    "IdealCurve_cumul_100=IdealCurve.groupby(level=[x for x in range(len(unique_identifier)+1)]).sum()\\\n",
    ".groupby(level=[x for x in range(1,len(unique_identifier)+1)])['lambda_100'].cumsum().reset_index().rename(columns={'lambda_100': 'lambda_100_cumsum'})\n",
    "IdealCurve_cumul_100['Dprio']=-IdealCurve_cumul_100['Dprio']\n",
    "\n",
    "IdealCurve_cumul_80=IdealCurve.groupby(level=[x for x in range(len(unique_identifier)+1)]).sum()\\\n",
    ".groupby(level=[x for x in range(1,len(unique_identifier)+1)])['lambda_80'].cumsum().reset_index().rename(columns={'lambda_80': 'lambda_80_cumsum'})\n",
    "IdealCurve_cumul_80['Dprio']=-IdealCurve_cumul_80['Dprio']\n",
    "\n",
    "IdealCurve=IdealCurve.reset_index()\n",
    "IdealCurve['Dprio']=-IdealCurve['Dprio']\n",
    "\n",
    "IdealCurve=IdealCurve.set_index(['Dprio']+unique_identifier)\n",
    "\n",
    "IdealCurve_cond_100=IdealCurve.groupby(level=[x for x in range(len(unique_identifier)+1)]).sum()\\\n",
    ".groupby(level=[x for x in range(1,len(unique_identifier)+1)])['lambda_100'].cumsum().reset_index().rename(columns={'lambda_100': 'lambda_100_cond'})\n",
    "\n",
    "IdealCurve_cond_80=IdealCurve.groupby(level=[x for x in range(len(unique_identifier)+1)]).sum()\\\n",
    ".groupby(level=[x for x in range(1,len(unique_identifier)+1)])['lambda_80'].cumsum().reset_index().rename(columns={'lambda_80': 'lambda_80_cond'})\n",
    "\n",
    "IdealCurve=IdealCurve.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "IdealCurve_cumul_100=IdealCurve_cumul_100.sort_values(by=['Dprio']+unique_identifier)\n",
    "IdealCurve_cumul_80=IdealCurve_cumul_80.sort_values(by=['Dprio']+unique_identifier)\n",
    "IdealCurve_cond_100=IdealCurve_cond_100.sort_values(by=['Dprio']+unique_identifier)\n",
    "IdealCurve_cond_80=IdealCurve_cond_80.sort_values(by=['Dprio']+unique_identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "TotalCap=TotalCap.merge(GroupPax[['Dprio']+unique_identifier+['Group_pax_cumul']],on=['Dprio']+unique_identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "TotalCap=TotalCap.merge(IdealCurve_cumul_80.loc[IdealCurve_cumul_80['Dprio']==1,unique_identifier+['lambda_80_cumsum']],on=unique_identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "TotalCap=TotalCap.rename(columns={'lambda_80_cumsum': 'Goal_80'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "TotalCap['C_100']=TotalCap['Cabin Capacity']-TotalCap['Group_pax_cumul']\n",
    "TotalCap['C_80']=TotalCap[['Goal_80','C_100']].apply(lambda x: min(x),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "IdealCurve_cumul_100=IdealCurve_cumul_100.merge(TotalCap[['Dprio']+unique_identifier+['Cabin Capacity','C_100']],\n",
    "on=['Dprio']+unique_identifier)\n",
    "IdealCurve_cond_100=IdealCurve_cond_100.merge(TotalCap[['Dprio']+unique_identifier+['Cabin Capacity','C_100']],\n",
    "on=['Dprio']+unique_identifier)\n",
    "IdealCurve_cumul_80=IdealCurve_cumul_80.merge(TotalCap[['Dprio']+unique_identifier+['Goal_80','C_80']],\n",
    "on=['Dprio']+unique_identifier)\n",
    "IdealCurve_cond_80=IdealCurve_cond_80.merge(TotalCap[['Dprio']+unique_identifier+['Goal_80','C_80']],\n",
    "on=['Dprio']+unique_identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "IdealCurve_cumul_100['lambda_100_cumsum']=IdealCurve_cumul_100['lambda_100_cumsum']*(IdealCurve_cumul_100['C_100']/IdealCurve_cumul_100['Cabin Capacity'])\n",
    "IdealCurve_cond_100['lambda_100_cond']=IdealCurve_cond_100['lambda_100_cond']*(IdealCurve_cond_100['C_100']/IdealCurve_cond_100['Cabin Capacity'])\n",
    "IdealCurve_cumul_80['lambda_80_cumsum']=IdealCurve_cumul_80['lambda_80_cumsum']*(IdealCurve_cumul_80['C_80']/IdealCurve_cumul_80['Goal_80'])\n",
    "IdealCurve_cond_80['lambda_80_cond']=IdealCurve_cond_80['lambda_80_cond']*(IdealCurve_cond_80['C_80']/IdealCurve_cond_80['Goal_80'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "IdealCurve_cond_100['thres_100']=poisson.ppf(1-risk_spoil,IdealCurve_cond_100['lambda_100_cond'])\n",
    "IdealCurve_cond_80['thres_80']=poisson.ppf(risk_spoil,IdealCurve_cond_80['lambda_80_cond'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "IdealCurve_cumul_100=IdealCurve_cumul_100.merge(IdealCurve_cond_100[['Dprio']+unique_identifier+['thres_100']],on=['Dprio']+unique_identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "IdealCurve_cumul_80=IdealCurve_cumul_80.merge(IdealCurve_cond_80[['Dprio']+unique_identifier+['thres_80']],on=['Dprio']+unique_identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "IdealCurve_cumul_100['Ideal_upper']=IdealCurve_cumul_100['C_100']-IdealCurve_cumul_100['thres_100']\n",
    "IdealCurve_cumul_100['Ideal_upper']=IdealCurve_cumul_100[['Ideal_upper','C_100']].apply(lambda x: x[1] if x[0]>x[1] else x[0],axis=1)\n",
    "IdealCurve_cumul_80['Ideal_lower']=IdealCurve_cumul_80['C_80']-IdealCurve_cond_80['thres_80']\n",
    "IdealCurve_cumul_80['Ideal_lower']=IdealCurve_cumul_80['Ideal_lower'].apply(lambda x: 0 if x<0 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "IdealCurve=IdealCurve_cumul_100.merge(IdealCurve_cumul_80,on=['Dprio']+unique_identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "IdealCurve=IdealCurve[['Dprio']+unique_identifier+['Ideal_lower','lambda_80_cumsum','lambda_100_cumsum','Ideal_upper']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "IdealCurve.columns=['Dprio']+unique_identifier+['Ramp-up frontier','Ideal curve (80% LF)','Ideal curve (100% LF)','Phase-down frontier']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Risk and Actual Bookings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "Risk=Prediction[['Dprio']+unique_identifier+['NumPax']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mapping_StrToDt=pd.concat([Series(Risk['DepDate'].unique()),\n",
    "Series([pd.to_datetime(x) for x in Risk['DepDate'].unique()])],axis=1)\n",
    "Mapping_StrToDt.columns=['DepDate','DepDate_new']\n",
    "Risk=Risk.merge(Mapping_StrToDt,on='DepDate')\n",
    "Risk['DepDate']=Risk['DepDate_new']\n",
    "Risk=Risk[[x for x in Risk.columns if x!='DepDate_new']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "Risk['Dprio']=-Risk['Dprio']\n",
    "Risk=Risk.set_index(['Dprio']+unique_identifier)\n",
    "\n",
    "Risk=Risk.groupby(level=[x for x in range(len(unique_identifier)+1)]).sum()\\\n",
    ".groupby(level=[x for x in range(1,len(unique_identifier)+1)])['NumPax'].cumsum().reset_index()\\\n",
    ".rename(columns={'NumPax': 'NumPax_cumsum'})\n",
    "\n",
    "Risk['Dprio']=-Risk['Dprio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "Risk['IssueDate']=Risk['DepDate']-Risk['Dprio'].apply(lambda x: datetime.timedelta(x-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermezzo for creating Actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "Actuals=Risk.loc[Risk['IssueDate']<=pd.to_datetime(datetime.datetime.today().strftime('%Y-%m-%d')),:]\n",
    "Actuals=Actuals[[x for x in Actuals.columns if x!='IssueDate']]\n",
    "Actuals=Actuals.rename(columns={'NumPax_cumsum': 'Actual Bookings'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue with Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "Risk=Risk.loc[Risk['IssueDate']==pd.to_datetime(datetime.datetime.today().strftime('%Y-%m-%d')),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "Risk=Risk[[x for x in Risk.columns if x!='IssueDate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "Risk['DepDate']=Risk['DepDate'].apply(lambda x: str(x).split(' ')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "Risk=Risk.merge(IdealCurve.loc[IdealCurve['Dprio']==1,unique_identifier+['Ideal curve (80% LF)','Ideal curve (100% LF)']]\\\n",
    ".rename(columns={'Ideal curve (100% LF)': 'C_100', 'Ideal curve (80% LF)': 'C_80'}),on=unique_identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "Risk=Risk.merge(IdealCurve_cond_100[['Dprio']+unique_identifier+['lambda_100_cond']],on=['Dprio']+unique_identifier)\\\n",
    ".merge(IdealCurve_cond_80[['Dprio']+unique_identifier+['lambda_80_cond']],on=['Dprio']+unique_identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "Risk['SpillageRisk']=1-poisson.cdf(Risk['C_100']-Risk['NumPax_cumsum'],Risk['lambda_100_cond'])\n",
    "Risk['SpoilageRisk']=poisson.cdf(Risk['C_80']-Risk['NumPax_cumsum'],Risk['lambda_80_cond'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "Risk=Risk.merge(IdealCurve[['Dprio']+unique_identifier+['Ramp-up frontier','Phase-down frontier']],\n",
    "          on=['Dprio']+unique_identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "Risk['Intensity_downweighted']=Risk['NumPax_cumsum']-Risk['Ramp-up frontier']\n",
    "Risk['Intensity_full']=Risk['NumPax_cumsum']-Risk['Phase-down frontier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "Risk=Risk[['DepDate','FltNum','dtime','SpoilageRisk','Intensity_downweighted','Intensity_full','SpillageRisk']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mapping_StrToDt=pd.concat([Series(TotalCap['DepDate'].unique()),Series([pd.to_datetime(x) for x in TotalCap['DepDate'].unique()])],axis=1)\n",
    "Mapping_StrToDt.columns=['DepDate','DepDate_new']\n",
    "TotalCap=TotalCap.merge(Mapping_StrToDt,on='DepDate')\n",
    "TotalCap['DepDate']=TotalCap['DepDate_new']\n",
    "\n",
    "TotalCap=TotalCap[[x for x in TotalCap.columns if x!='DepDate_new']]\n",
    "\n",
    "TotalCap=TotalCap.loc[TotalCap['DepDate']>=pd.to_datetime('2019-04-30'),]\n",
    "\n",
    "TotalCap['DepDate']=TotalCap['DepDate'].apply(lambda x: str(x).split(' ')[0])\n",
    "\n",
    "TotalCap=TotalCap.rename(columns={'Cabin Capacity': 'Initial Cabin Capacity'})\n",
    "TotalCap=TotalCap.rename(columns={'C_100': 'Real Cabin Capacity'})\n",
    "\n",
    "TotalCap.to_csv(path+'FrontEnd_Input/Capacity_'+route+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "Risk=Risk.merge(Prediction[unique_identifier+['Direction','month','dday']].drop_duplicates(subset=unique_identifier),on=unique_identifier)\n",
    "Risk['Route']=route\n",
    "\n",
    "MonthMapping=DataFrame([[x for x in range(1,13)],['January','February','March','April','May','June','July','August',\n",
    "                'September','October','November','December']]).transpose()\n",
    "MonthMapping.columns=['month','Month']\n",
    "MonthMapping['month']=MonthMapping['month'].astype('int') \n",
    "Risk=Risk.merge(MonthMapping,on='month')\n",
    "Risk['month']=Risk['Month']\n",
    "Risk=Risk[[x for x in Risk.columns if x!='Month']]\n",
    "\n",
    "Risk=Risk[['Route','DepDate', 'FltNum', 'dtime', 'Direction', 'month', 'dday',\n",
    "       'SpoilageRisk', 'Intensity_downweighted', 'Intensity_full',\n",
    "       'SpillageRisk']]\n",
    "\n",
    "Risk=Risk.sort_values(by=['DepDate', 'FltNum', 'dtime', 'Direction'])\n",
    "\n",
    "Risk.to_csv(path+'FrontEnd_Input/Risk_'+route+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "Actuals['DepDate']=Actuals['DepDate'].apply(lambda x: str(x).split(' ')[0])\n",
    "\n",
    "Actuals=Actuals.merge(Prediction[unique_identifier+['Direction','month','dday']].drop_duplicates(subset=unique_identifier),\n",
    "on=unique_identifier)\n",
    "Actuals['Route']=route\n",
    "\n",
    "Actuals=Actuals[['Route','DepDate', 'FltNum', 'dtime', 'Direction', 'month', 'dday', 'Dprio','Actual Bookings']]\n",
    "\n",
    "Actuals['Actual Bookings']=Actuals['Actual Bookings'].apply(lambda x: x if x>=0 else 0)\n",
    "\n",
    "Actuals.to_csv(path+'FrontEnd_Input/Actuals_'+route+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "IdealCurve=IdealCurve.merge(Prediction[unique_identifier+['Direction','month','dday']].drop_duplicates(subset=unique_identifier),\n",
    "on=unique_identifier)\n",
    "\n",
    "IdealCurve['Route']=route\n",
    "\n",
    "IdealCurve=IdealCurve[['Route','DepDate', 'FltNum', 'dtime', 'Direction', 'month', 'dday', 'Dprio',\n",
    "'Ramp-up frontier','Ideal curve (80% LF)','Ideal curve (100% LF)','Phase-down frontier']]\n",
    "\n",
    "IdealCurve.to_csv(path+'FrontEnd_Input/IdealCurve_'+route+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
